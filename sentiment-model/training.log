2026-01-19 10:51:51,375 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,375 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): DistilBertSdpaAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2026-01-19 10:51:51,376 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,376 Corpus: 72 train + 8 dev + 20 test sentences
2026-01-19 10:51:51,376 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,377 Train:  72 sentences
2026-01-19 10:51:51,377         (train_with_dev=False, train_with_test=False)
2026-01-19 10:51:51,377 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,378 Training Params:
2026-01-19 10:51:51,378  - learning_rate: "0.1" 
2026-01-19 10:51:51,378  - mini_batch_size: "16"
2026-01-19 10:51:51,379  - max_epochs: "5"
2026-01-19 10:51:51,379  - shuffle: "True"
2026-01-19 10:51:51,379 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,379 Plugins:
2026-01-19 10:51:51,380  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'
2026-01-19 10:51:51,381 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,381 Final evaluation on model from best epoch (best-model.pt)
2026-01-19 10:51:51,381  - metric: "('micro avg', 'f1-score')"
2026-01-19 10:51:51,381 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,382 Computation:
2026-01-19 10:51:51,382  - compute on device: cpu
2026-01-19 10:51:51,382  - embedding storage: cpu
2026-01-19 10:51:51,383 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,383 Model training base path: "sentiment-model"
2026-01-19 10:51:51,383 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:51,384 ----------------------------------------------------------------------------------------------------
2026-01-19 10:51:59,550 epoch 1 - iter 1/5 - loss 0.33876565 - time (sec): 8.17 - samples/sec: 1.96 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:06,641 epoch 1 - iter 2/5 - loss 3.36136676 - time (sec): 15.26 - samples/sec: 2.10 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:13,364 epoch 1 - iter 3/5 - loss 3.21346681 - time (sec): 21.98 - samples/sec: 2.18 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:20,204 epoch 1 - iter 4/5 - loss 2.61901397 - time (sec): 28.82 - samples/sec: 2.22 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:23,209 epoch 1 - iter 5/5 - loss 2.66366456 - time (sec): 31.82 - samples/sec: 2.26 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:23,210 ----------------------------------------------------------------------------------------------------
2026-01-19 10:52:23,210 EPOCH 1 done: loss 2.6637 - lr: 0.100000
2026-01-19 10:52:23,854 DEV : loss 0.7869663238525391 - f1-score (micro avg)  0.7619
2026-01-19 10:52:23,857  - 0 epochs without improvement
2026-01-19 10:52:23,857 saving best model
2026-01-19 10:52:24,305 ----------------------------------------------------------------------------------------------------
2026-01-19 10:52:31,772 epoch 2 - iter 1/5 - loss 0.70868117 - time (sec): 7.47 - samples/sec: 2.14 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:38,891 epoch 2 - iter 2/5 - loss 1.62551609 - time (sec): 14.58 - samples/sec: 2.19 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:45,942 epoch 2 - iter 3/5 - loss 1.31362667 - time (sec): 21.64 - samples/sec: 2.22 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:54,516 epoch 2 - iter 4/5 - loss 1.62064414 - time (sec): 30.21 - samples/sec: 2.12 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:57,833 epoch 2 - iter 5/5 - loss 1.53165752 - time (sec): 33.53 - samples/sec: 2.15 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:52:57,834 ----------------------------------------------------------------------------------------------------
2026-01-19 10:52:57,834 EPOCH 2 done: loss 1.5317 - lr: 0.100000
2026-01-19 10:52:57,853 DEV : loss 3.568312168121338 - f1-score (micro avg)  0.7273
2026-01-19 10:52:57,856  - 1 epochs without improvement
2026-01-19 10:52:57,857 ----------------------------------------------------------------------------------------------------
2026-01-19 10:53:05,756 epoch 3 - iter 1/5 - loss 2.35830307 - time (sec): 7.90 - samples/sec: 2.03 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:12,897 epoch 3 - iter 2/5 - loss 1.48057100 - time (sec): 15.04 - samples/sec: 2.13 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:19,947 epoch 3 - iter 3/5 - loss 1.64821237 - time (sec): 22.09 - samples/sec: 2.17 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:27,285 epoch 3 - iter 4/5 - loss 1.37937646 - time (sec): 29.43 - samples/sec: 2.17 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:30,566 epoch 3 - iter 5/5 - loss 1.34820735 - time (sec): 32.71 - samples/sec: 2.20 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:30,566 ----------------------------------------------------------------------------------------------------
2026-01-19 10:53:30,567 EPOCH 3 done: loss 1.3482 - lr: 0.100000
2026-01-19 10:53:30,582 DEV : loss 1.0145461559295654 - f1-score (micro avg)  0.7273
2026-01-19 10:53:30,584  - 2 epochs without improvement
2026-01-19 10:53:30,586 ----------------------------------------------------------------------------------------------------
2026-01-19 10:53:38,059 epoch 4 - iter 1/5 - loss 0.25976077 - time (sec): 7.47 - samples/sec: 2.14 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:45,300 epoch 4 - iter 2/5 - loss 1.32193024 - time (sec): 14.71 - samples/sec: 2.17 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:52,485 epoch 4 - iter 3/5 - loss 0.96885264 - time (sec): 21.90 - samples/sec: 2.19 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:53:59,834 epoch 4 - iter 4/5 - loss 0.77624779 - time (sec): 29.25 - samples/sec: 2.19 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:02,930 epoch 4 - iter 5/5 - loss 0.69357328 - time (sec): 32.34 - samples/sec: 2.23 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:02,931 ----------------------------------------------------------------------------------------------------
2026-01-19 10:54:02,931 EPOCH 4 done: loss 0.6936 - lr: 0.100000
2026-01-19 10:54:02,945 DEV : loss 2.2606911659240723 - f1-score (micro avg)  0.7273
2026-01-19 10:54:02,947  - 3 epochs without improvement
2026-01-19 10:54:02,948 ----------------------------------------------------------------------------------------------------
2026-01-19 10:54:10,149 epoch 5 - iter 1/5 - loss 3.85622931 - time (sec): 7.20 - samples/sec: 2.22 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:17,712 epoch 5 - iter 2/5 - loss 2.18180665 - time (sec): 14.76 - samples/sec: 2.17 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:25,106 epoch 5 - iter 3/5 - loss 1.53397528 - time (sec): 22.16 - samples/sec: 2.17 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:32,564 epoch 5 - iter 4/5 - loss 1.19401234 - time (sec): 29.61 - samples/sec: 2.16 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:35,896 epoch 5 - iter 5/5 - loss 1.11950923 - time (sec): 32.95 - samples/sec: 2.19 - lr: 0.100000 - momentum: 0.000000
2026-01-19 10:54:35,897 ----------------------------------------------------------------------------------------------------
2026-01-19 10:54:35,901 EPOCH 5 done: loss 1.1195 - lr: 0.100000
2026-01-19 10:54:35,920 DEV : loss 1.885100245475769 - f1-score (micro avg)  0.7273
2026-01-19 10:54:35,922  - 4 epochs without improvement (above 'patience')-> annealing learning_rate to [0.05]
2026-01-19 10:54:36,436 ----------------------------------------------------------------------------------------------------
2026-01-19 10:54:36,436 Loading model from best epoch ...
2026-01-19 10:54:39,616 
Results:
- F-score (micro) 0.7692
- F-score (macro) 0.5
- Accuracy 0.4

By class:
              precision    recall  f1-score   support

    POSITIVE     1.0000    1.0000    1.0000        20
    NEGATIVE     0.0000    0.0000    0.0000        12

   micro avg     1.0000    0.6250    0.7692        32
   macro avg     0.5000    0.5000    0.5000        32
weighted avg     0.6250    0.6250    0.6250        32
 samples avg     1.0000    0.7000    0.8000        32

2026-01-19 10:54:39,616 ----------------------------------------------------------------------------------------------------
